{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you are going to predict house values with another model.\n",
    "\n",
    "Go through the following steps. Your solution should be in a Python notebook, including all the steps described here in the following.\n",
    "\n",
    "### 1. Load Dataset\n",
    "\n",
    "Read the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('housing.csv')\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Data Into Train and Test Sets\n",
    "\n",
    "Split the dataset into train and test sets we can use to evaluate our model. This step also helps prevent data leakages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    "\n",
    "Adding additional features that will help improve the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"rooms_per_household\"] = train[\"total_rooms\"]/train[\"households\"]\n",
    "train[\"bedrooms_per_room\"] = train[\"total_bedrooms\"]/train[\"total_rooms\"]\n",
    "train[\"population_per_household\"]=train[\"population\"]/train[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Scaling\n",
    "\n",
    "Handle missing values and re-scale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the label values in a new series\n",
    "train_labels = train['median_house_value'].copy()\n",
    "\n",
    "# Drop the label column from the rest of the dataframe\n",
    "train =  train.drop('median_house_value', axis=1)\n",
    "\n",
    "# Separate numerical fields in the test dataset\n",
    "train_num = train.drop(\"ocean_proximity\", axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline to impute missing values with the median value and scale the data using the standard scaler\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")), ('std_scaler', StandardScaler())])\n",
    "\n",
    "num_attribs = list(train_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Additional pipline to add a step to previous pipeline that encode categorical values\n",
    "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs)])\n",
    "\n",
    "# Run the final pipeline\n",
    "train_prepared = full_pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "sgd_reg.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cross-Validation\n",
    "\n",
    "Now that the model is trained, let’s evaluate it on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(sgd_reg, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "sgd_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_rmse_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(sgd_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Tuning\n",
    "\n",
    "Now let's tune the model with a set of hyperparameters to see if we can improve it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'eta0': [0.0001, 0.005, 0.001, 0.01], 'power_t': [0.35, 0.3, 0.25], 'max_iter': [2000, 4000, 6000]}, \n",
    "    ]\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(sgd_reg, param_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\n",
    "\n",
    "grid_search.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ \n",
    "\n",
    "# Our result {'eta0': 0.0001, 'max_iter': 6000, 'power_t': 0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Testing\n",
    "\n",
    "Let's evaluate our model against our test set so we can compare it's overall performance to our previous models.\n",
    "\n",
    "First we prepare the data. Then we use our model to predict and finally evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"rooms_per_household\"] = test[\"total_rooms\"]/test[\"households\"]\n",
    "test[\"bedrooms_per_room\"] = test[\"total_bedrooms\"]/test[\"total_rooms\"]\n",
    "test[\"population_per_household\"]=test[\"population\"]/test[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the label values in a new series\n",
    "test_labels = test['median_house_value'].copy()\n",
    "\n",
    "# Drop the label column from the rest of the dataframe\n",
    "test =  test.drop('median_house_value', axis=1)\n",
    "\n",
    "# Separate numerical fields in the test dataset\n",
    "test_num = test.drop(\"ocean_proximity\", axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared = full_pipeline.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sgd_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_sgd_model.predict(test_prepared)\n",
    "\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_mse = mean_squared_error(test_labels, test_predictions)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "\n",
    "model_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully built our model and can now compare the test result to estimate which model would be better for production. \n",
    "\n",
    "Our previous model (random forest model) had a better RMSE score (66940.81) than this model (69250.28) so it is safe to pick that model over this. Although we woudl not be using this model, this has not been a waste of time. The errors from both models are still too high and we can still spend more time tuning the hyperparameters in order to reduce it for both models. \n",
    "\n",
    "We can also try out more complex models to help achieve this goal. Maybe you can try a more complex tree-based algorithm to improve the our error scores.\n",
    "\n",
    "\n",
    "### 5.4 Production\n",
    "\n",
    "The next step after obtaining an optimal model is using it in a production environment. To do that we would have to save our preferred model and serve it usually trough a website where it can take input and make predictions.\n",
    "\n",
    "Remember we performed some data preprocessing and feature engineering and scaling on our data and would have to replicate these steps for any input we intend to feed to our model in the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "# filename = 'sgd_housing_model.pkl'\n",
    "\n",
    "#joblib.dump(grid_search.best_estimator_,filename)\n",
    "\n",
    "filename = 'sgd_housing_model.sav'\n",
    "pickle.dump(grid_search.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want re-load the model you can use the following code:\n",
    "\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "9127ab915aeeef02e88f7c7310ecae586ccb51d77b915ed104f093bc6474a7bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
